{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This lab will introduce methods of classification, specifically **Nearest-Neighbor** classification and **Support Vector Machines**. It will also provide experience with working with high dimensional data, the computational challenges of such data, and the use of hashing to allieviate computational burdens of high-dimensional data. \n",
      "\n",
      "Python files are provided with the lab for each exercise, which include code to generate the necessary datasets. \n",
      "\n",
      "1. Please *make the necessary modifications* to this code as specified in the exercises (which are reiterated in the comments of the code) and submit these files as part of your report. \n",
      "2. You can either submit *this code and a report*, or copy the code into an IPython notebook and include any necessary text or explanations in the notebook as your report. Lab reports and code are due on *Friday* following the lab at **11:59 p.m**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Nearest-Neighbor and K-Means in High Dimensions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Nearest-Neighbor (NN) can be an effective classification tool, especially when the data is *not separable* by a linear classifier. It requires no parameter tuning, except for in the case of k-NN, which searches for the \"k\" nearest neighbors, where k is some constant, usually small. \n",
      "* However, a simple implementation of NN would perform a brute-force search over all possible points to find the nearest neighbor, which can be computationally demanding in high dimensions, as we will see.\n",
      "\n",
      "* Similarly, the computational cost of K-means increases significantly as the dimensionality of the data increases, which you will discover in the exercise."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In classification problems, there are two datasets: a *test set* and a *training set*. Both are generated from the same source (in this case, a high-dimensional Gaussian distribution), but one is used to train the classifier and one is used to test it. We will generate **two clusters**, or classes, of Gaussian data in high dimensions (1 million dimensions). There will be some overlap of the clusters, so the two classes will not be perfectly separable by a linear decision boundary. First, we generate the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "D = 1000000                   # dimensionality of the data\n",
      "\n",
      "N_train = 100                 # number of training data points\n",
      "\n",
      "# Generate Gaussian training data in D dimensions\n",
      "mu = [0, 1]\n",
      "sigma = 43\n",
      "X_train = np.vstack((np.random.normal(mu[0], sigma, (N_train,D)).astype(np.float32), \\\n",
      "    np.random.normal(mu[1], sigma, (N_train,D)).astype(np.float32)))\n",
      "# Generate training labels\n",
      "Y_train = np.hstack((np.tile([0], [1, N_train])[0,:], np.tile([1], [1, N_train])[0,:]))\n",
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "Y_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we will generate the test data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate Gaussian testing data in D dimensions\n",
      "N_test = 50\n",
      "X_test = np.vstack((np.random.normal(mu[0], sigma, (N_test,D)).astype(np.float32), \\\n",
      "    np.random.normal(mu[1], sigma, (N_test,D)).astype(np.float32)))\n",
      "# Generate testing labels\n",
      "Y_test = np.hstack((np.tile([0], [1, N_test])[0,:], np.tile([1], [1, N_test])[0,:]))\n",
      "\n",
      "X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We cannot visualize high-dimensional data in its entirety, but we can look at two dimensions at a time. We'll use the same plot_2D function defined in the last lab and the following code to do so."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import cycle\n",
      "\n",
      "def plot_2D(data, target, target_names):\n",
      "    colors = colors = ['b', '#ff9000']             # colors for plot\n",
      "    target_ids = range(len(target_names))\n",
      "    plt.figure()\n",
      "    for i, c, label in zip(target_ids, colors, target_names):\n",
      "        plt.scatter(data[target == i, 0], data[target == i, 1], c=c, label=label)\n",
      "    plt.legend()                                   # add a legend"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Plot first two dimensions of training data\n",
      "plot_2D(X_train[:,0:2], Y_train, [\"c0\", \"c1\"])\n",
      "plt.title(\"2D Gaussian Data with Overlap (Training)\")\n",
      "plt.show()\n",
      "\n",
      "# Plot first two dimensions of testing data\n",
      "plot_2D(X_test[:,0:2], Y_test, [\"c0\", \"c1\"])\n",
      "plt.title(\"2D Gaussian Data with Overlap (Testing)\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* In NN classification, each test point is labeled by assigning to it the label of its **closest neighbor** in the training set. In k-NN, the majority label of the k nearest neighbors is assigned. \n",
      "* The Python library scikit-learn provides an implementation of nearest neighbor search. We can use it to perform NN classification on the dataset. \n",
      "* The simplest form of NN search is to use \"brute force,\" which is to search over **every point** in the dataset for the exact nearest neighbor. However, this can be infeasible in high dimensions. \n",
      "* Other algorithms have been proposed for NN search that make more efficient use of computation, such as the **KD-Tree** approach. Notice the time difference between the two approaches below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import NearestNeighbors\n",
      "import time\n",
      "\n",
      "t = time.time()                   # function for retrieving the time\n",
      "# Brute force NN\n",
      "nbrs = NearestNeighbors(n_neighbors=1, algorithm='brute').fit(X_train)\n",
      "distances, indices = nbrs.kneighbors(X_test)\n",
      "print time.time() - t             # time in seconds\n",
      "\n",
      "t = time.time()                   # function for retrieving the time\n",
      "# KD Tree NN\n",
      "nbrs = NearestNeighbors(n_neighbors=1, algorithm='kd_tree').fit(X_train)\n",
      "distances, indices = nbrs.kneighbors(X_test)\n",
      "print time.time() - t             # time in seconds\n",
      "\n",
      "# Compute accuracy\n",
      "Y_nn = Y_train[indices[:,0]]\n",
      "result = Y_nn == Y_test\n",
      "percent = 100*sum(result)/(2.*N_test)\n",
      "print percent                     # classification accuracy (in percent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implement Nearest Neighbor classification to test on the high-dimensional Gaussian data generated by the above code. For each point in the test set, find its nearest neighbor in the training set and assign it the same class label as its nearest neighbor. Record the classification accuracy and computation time needed to perform classification.\n",
      "\n",
      "Also run K-means clustering on the data and record the computation time and clustering accuracy. \n",
      "\n",
      "Note: Since K-means is an \"unsupervised\" algorithm (it does not make use of training labels), it will assign arbitrary labels to the learned clusters. Therefore, if the accuracy of the clustering is low, it is likely that K-means has flipped the labels of the clusters. In that case, simply flip the labels and output the resulting accuracy.\n",
      "\n",
      "Some useful commands: scipy.spatial.distance.cdist(), numpy.argmin(), time.time()\n",
      "\n",
      "**Hint**: An implementation of brute-force NN search should only require approximately 4 lines of code in Python."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.spatial.distance\n",
      "\n",
      "def nneighbor(X, x):\n",
      "    t = time.time() \n",
      "    # your implementation of NN\n",
      "    # X is the training data (the set of candidate neighbors) and x is the testing\n",
      "    # data which is to be labeled\n",
      "    d = scipy.spatial.distance.cdist(X,x) #This creates a 200 by 100 matrix that contains all the pairwise distances\n",
      "    ind = np.argmin(d,0) #this gets the minimum value indexes of d matrix. i is a 100 dimensional matrix\n",
      "    X_nn = X[ind,:] #The index gives the row-wise ind point of X which gives minimum distance      \n",
      "    print 'Time for Nearest Neighbor',time.time() - t  \n",
      "    return [X_nn, ind]\n",
      "    # X_nn is the matrix of nearest neighbors for the input query x\n",
      "    # ind are the indices in X of the nearest neighbors\n",
      "    \n",
      "X_nn,indeces = nneighbor(X_train,X_test)\n",
      "\n",
      "Y_nn = Y_train[indices[:,0]]\n",
      "result = Y_nn == Y_test\n",
      "percent = 100*sum(result)/(2.*N_test)\n",
      "print 'Nearest Neightbor implementation accuracy:',percent \n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "t = time.time()\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "kmeans.fit(X_test)\n",
      "\n",
      "Y_NN = kmeans.labels_\n",
      "result1 = Y_NN == Y_test\n",
      "percent1 = 100*sum(result1)/(2.*N_test) #flipped % because it was too low (flipped labels)\n",
      "print 'Accuracy of Kmeans:',percent1 \n",
      "print 'Time taken for Kmeans:',time.time() -t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Hashing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now see how hashing can help to reduce the computational complexity of some of the learning tools we have introduced so far. In class, we discussed hashing using random projections. Implement the hashing function as described in lecture and generate the hashed versions of the data from the previous section. Try a spectrum of values for the reduced dimension (10000, 5000, 1000, 500, 100). Rerun NN classification and K-means on the hashed data. Report the timing and classification results for both tools and compare with the results of the previous section."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hashing(X,rd)\n",
      "t = time.time()\n",
      "mu = 0\n",
      "sigma = 0.1\n",
      "u = np.random.normal(mu,sigma,(rd,D))\n",
      "w = np.dot(x,u.T)\n",
      "b = 0.5*np.sign(w + 1)\n",
      "print 'Time taken to hash:',time.time() -t\n",
      "return b\n",
      "\n",
      "b_test = hashing(X_test,2)\n",
      "b_train = hashing(X_train,2)\n",
      "\n",
      "from scipy.cluster.vq import *\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "t1 = time.time()\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "kmeans.fit(b_test)\n",
      "\n",
      "Y_nn = kmeans.labels_\n",
      "result = Y_nn == Y_test\n",
      "percent = 100*sum(result)/(2.*N_test)\n",
      "print 'Accuracy of Hashed Kmeans:',percent\n",
      "print 'Time taken to do Kmeans of hashed data',time.time() - t1\n",
      "\n",
      "t2 = time.time()                   # function for retrieving the time\n",
      "nbrs = NearestNeighbors(n_neighbors=1, algorithm='brute').fit(b_train)\n",
      "distances, indices = nbrs.kneighbors(b_test)\n",
      "print \"Scikit-learn NN - Brute Force:\"\n",
      "print 'Time taken to do NN-Brute Force:',time.time() - t2             # time in seconds\n",
      "\n",
      "Y_Nn = Y_train[indices[:,0]]\n",
      "result1 = Y_Nn == Y_test\n",
      "percent1 = 100*sum(result)/(2.*N_test)\n",
      "print 'Accuracy of NN-Brute Force:',percent1 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Support Vector Machines (SVM) is one of the most popular linear classifiers for classification tasks. In the perfectly linearly separable case, SVM creates a linear decision boundary that maximizes the margin between it and the closest points for each class. It can also be applied to problems that are not perfectly linearly separable using an extension of SVM called \"soft margin\" SVM."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll first look at SVM applied to 2D Gaussian data. First, generate the 2D Gaussian data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define mean and covariance of Gaussian data\n",
      "D = 2\n",
      "mean1 = np.tile([0], [1, D])[0,:]\n",
      "mean2 = np.tile([1], [1, D])[0,:]\n",
      "cov = np.eye(D)*(0.25/np.sqrt(D))\n",
      "\n",
      "# Generate training data\n",
      "N_svm_train = 100\n",
      "X1 = np.random.multivariate_normal(mean1, cov, N_svm_train)    # create multivariate Gaussian data\n",
      "X2 = np.random.multivariate_normal(mean2, cov, N_svm_train)    \n",
      "X_svm_train = np.vstack((X1,X2))\n",
      "Y_svm_train = np.hstack((np.tile([0], [1, N_svm_train])[0,:], np.tile([1], [1, N_svm_train])[0,:]))\n",
      "\n",
      "# Generate testing data\n",
      "N_svm_test = 50\n",
      "X1 = np.random.multivariate_normal(mean1, cov, N_svm_test)    # create multivariate Gaussian data\n",
      "X2 = np.random.multivariate_normal(mean2, cov, N_svm_test)    \n",
      "X_svm_test = np.vstack((X1,X2))\n",
      "Y_svm_test = np.hstack((np.tile([0], [1, N_svm_test])[0,:], np.tile([1], [1, N_svm_test])[0,:]))\n",
      "\n",
      "# Plot the training data\n",
      "plot_2D(X_svm_train, Y_svm_train, [\"c0\", \"c1\"])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scikit-learn also has an implementation of SVM. We can use it to classify the data. First, we must train the weights of the linear decision boundary using the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "\n",
      "clf = svm.SVC(kernel='linear')\n",
      "clf.fit(X_svm_train, Y_svm_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w = clf.coef_[0]          # the vector that defines the line of the decision boundary\n",
      "print w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can plot the training data and the learned classifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fancy code to generate points along line for plotting\n",
      "a = -w[0] / w[1]\n",
      "xx = np.linspace(-2, 3)\n",
      "yy = a * xx - (clf.intercept_[0]) / w[1]\n",
      "\n",
      "# Plot decision boundary overlayed on training data\n",
      "colors = ['b', '#ff9000']\n",
      "target_names = [\"c0\", \"c1\"]\n",
      "target_ids = range(len(target_names))\n",
      "data = X_svm_train[:,0:2]\n",
      "plt.figure()\n",
      "target = Y_svm_train\n",
      "for i, c, label in zip(target_ids, colors, target_names):\n",
      "    plt.scatter(data[target == i, 0], data[target == i, 1], c=c, label=label)\n",
      "plt.plot(xx, yy, 'k-')\n",
      "plt.legend()\n",
      "plt.title(\"SVM for 2D Non-separable Gaussian Data\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we classify the testing data using the learned SVM classifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.predict(X_svm_test)              # predict the labels of the testing data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute classification accuracy of SVM for 2D Gaussian data\n",
      "result = clf.predict(X_svm_test) == Y_svm_test\n",
      "percent = 100*sum(result)/(2*N_svm_test)\n",
      "print percent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise 3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use SVM on the data set from Exercise 1 and compare the results, both in terms of classification accuracy and computation time, to that of Nearest Neighbor classification. Why do you think that one performs better than the other (compare both accuracy and computation time)?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = svm.SVC(kernel='linear')\n",
      "clf.fit(X_train, Y_train)\n",
      "\n",
      "w = clf.coef_[0]          # the vector that defines the line of the decision boundary\n",
      "print w\n",
      "\n",
      "# Fancy code to generate points along line for plotting\n",
      "a = -w[0] / w[1]\n",
      "xx = np.linspace(-2, 3)\n",
      "yy = a * xx - (clf.intercept_[0]) / w[1]\n",
      "\n",
      "# Plot decision boundary overlayed on training data\n",
      "colors = ['b', '#ff9000']\n",
      "target_names = [\"c0\", \"c1\"]\n",
      "target_ids = range(len(target_names))\n",
      "data = X_train[:,0:2]\n",
      "plt.figure()\n",
      "target = Y_train\n",
      "for i, c, label in zip(target_ids, colors, target_names):\n",
      "    plt.scatter(data[target == i, 0], data[target == i, 1], c=c, label=label)\n",
      "plt.plot(xx, yy, 'k-')\n",
      "plt.legend()\n",
      "plt.title(\"SVM for 2D Non-separable Gaussian Data\")\n",
      "\n",
      "clf.predict(X_test)              # predict the labels of the testing data\n",
      "\n",
      "# Compute classification accuracy of SVM for 2D Gaussian data\n",
      "result = clf.predict(X_test) == Y_test\n",
      "percent = 100*sum(result)/(2*N_test)\n",
      "print percent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
